{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imperative Programming\n",
    "=================\n",
    "\n",
    "Many problems don't fit cleanly into `ndarray` or `DataFrame` abstractions.  How can we use dask to parallelize more custom workloads?\n",
    "\n",
    "We can always fall back to creating dictionaries manually:\n",
    "\n",
    "    dsk = {'load-1': (load, filename1), 'clean-1': (clean, 'load-1'), ...,\n",
    "           'load-2': (load, filename2), 'clean-2': (clean, 'load-2'), ...,\n",
    "           ...}\n",
    "    \n",
    "Manual dictionary creation though can be tedious, is prone to programmer error, and feels foreign to many developers. \n",
    "\n",
    "The dask `do` function helps you to construct custom dask graphs using more typical coding styles than the explicit construction of a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom graphs with `do`\n",
    "\n",
    "The `do` function delays a function evaluation, producing a lazily evaluated result.  One wraps a function with a `do` call\n",
    "\n",
    "*  Before:  \n",
    "\n",
    "        result = f(a, b, c=10)\n",
    "*  After:  \n",
    "\n",
    "        result = do(f)(a, b, c=10)\n",
    "        \n",
    "The result of a call to `do(function)` is a lazy `Value` object that we can use in future `do` calls or eventually call `.compute()`\n",
    "\n",
    "    >>> result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Familiar Example\n",
    "\n",
    "To explore this abstraction we revisit our examples from the [Foundations Notebook](02-Foundations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    return x\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "a = 1\n",
    "b = inc(a)\n",
    "\n",
    "x = 10\n",
    "y = inc(x)\n",
    "\n",
    "z = add(b, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally we parallelized this by constructing a dask graph explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsk = {'a': 1, \n",
    "       'b': (inc, 'a'),\n",
    "       \n",
    "       'x': 10,\n",
    "       'y': (inc, 'x'),\n",
    "       \n",
    "       'z': (add, 'b', 'y')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also use the `do` function to construct the dask graph with more traditional programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask import do\n",
    "\n",
    "a = 1\n",
    "b = do(inc)(a)\n",
    "\n",
    "x = 10\n",
    "y = do(inc)(x)\n",
    "\n",
    "z = do(add)(b, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These value objects build up the dask graph as they go.  These graphs are less interpretable but fine for normal execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "Consider our first exercise reading three CSV files with `pd.read_csv` and then measuring their total length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "filenames = [os.path.join('data', 'accounts.%d.csv' % i) for i in [0, 1, 2]]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "a = pd.read_csv(filenames[0])\n",
    "b = pd.read_csv(filenames[1])\n",
    "c = pd.read_csv(filenames[2])\n",
    "\n",
    "na = len(a)\n",
    "nb = len(b)\n",
    "nc = len(c)\n",
    "\n",
    "total = sum([na, nb, nc])\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first notebook we constructed a dask graph from this computation and then executed it in parallel using multiple processes to get a speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/Foundations-01.py\n",
    "dsk = {'a': (pd.read_csv, filenames[0]),\n",
    "       'b': (pd.read_csv, filenames[1]),\n",
    "       'c': (pd.read_csv, filenames[2]),\n",
    "       'na': (len, 'a'),\n",
    "       'nb': (len, 'b'),\n",
    "       'nc': (len, 'c'),\n",
    "       'total': (sum, ['na', 'nb', 'nc'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.multiprocessing import get\n",
    "%time  get(dsk, 'total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to recreate this graph again using the `do` function on the original Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = do(pd.read_csv)(filenames[0])\n",
    "...\n",
    "\n",
    "total = ...\n",
    "\n",
    "%time total.compute(get=get) # use multiprocessing get function in call to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "%load solutions/Imperative-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "Below is a function that approximates Pi using a [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method). It works by generating random points in a 1 x 1 square, and then counting those that are inside a quarter circle of radius one (as seen in [this gif](https://en.wikipedia.org/wiki/Monte_Carlo_method#/media/File:Pi_30K.gif)). Since the area of the full circle is Pi, then this can be estimated by 4 x points_in_circle/total_points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from random import random\n",
    "\n",
    "def estimate_pi(nsamples):\n",
    "    total = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random()\n",
    "        y = random()\n",
    "        if x*x + y*y <= 1:\n",
    "            total += 1\n",
    "    return 4.*total/nsamples\n",
    "\n",
    "estimate_pi(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to parallelize this computation using the `do` function.\n",
    "\n",
    "\n",
    "Think about what can be done in parallel here. The computation can be broken down into:\n",
    "\n",
    "1. Generate a bunch of random points\n",
    "2. For each point, determine if it's in the circle. If so, increment total counter.\n",
    "3. Return 4 * total/nsamples\n",
    "    \n",
    "This is basically a map operation (for each point do something), followed by a reduction (final aggregation arithmetic), so it can be done in an extremely parallel fashion, with the calculation for each point being a task. Below is a skeleton of a parallel version of the code, with the computation for each point missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@do\n",
    "def point_chunk():\n",
    "    \"\"\"Generates a random x, y point, returns 1 if in circle, else returns 0.\"\"\"\n",
    "    ...\n",
    "\n",
    "def parallel_estimate_pi(nsamples):\n",
    "    points = []\n",
    "    for i in range(nsamples):\n",
    "        points.append(point_chunk())\n",
    "    total = do(sum)(points)\n",
    "    result = 4.*total/nsamples\n",
    "    return result.compute(get=get)\n",
    "\n",
    "parallel_estimate_pi(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time estimate_pi(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time parallel_estimate_pi(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "%load solutions/Imperative-02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How fast did that run? Was it faster or slower than the serial version? \n",
    "\n",
    "Even though there was a high amount of parallelism here, it ran significantly slower. This is because of the overhead of the dask schedulers. Dask is really good at \"large grain parallelism\", but when the task size gets to be too small then the scheduler overhead becomes a problem. \n",
    "\n",
    "This can be fixed by partioning the computation into larger blocks. Below is another take on the same computation. This time we partition the total number of samples into `npartitions` using the provided `partition` function. We then apply `total_chunk` to each partition, sum the results, then apply the arithmetic to calculate Pi. In this way, each chunk is more computation heavy, so the overhead from the schedulers is smaller. Finish up the implementation by writing `total_chunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partition(num, npartitions):\n",
    "    \"\"\"Partition `num` into `npartitions` partitions.\n",
    "    \n",
    "    >>> partition(10, 3)\n",
    "    [3, 3, 4]\n",
    "    \"\"\"\n",
    "    parts = [num//npartitions] * npartitions\n",
    "    parts[-1] += num%npartitions\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@do\n",
    "def total_chunk(nsamples):\n",
    "    \"\"\"Generates `nsamples` random x, y points, returns number of \n",
    "    points that were in the circle.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "def parallel_estimate_pi(nsamples, npartitions):\n",
    "    totals = []\n",
    "    for n in partition(nsamples, npartitions):\n",
    "        totals.append(total_chunk(n))\n",
    "    result = 4.*do(sum)(totals)/nsamples\n",
    "    return result.compute(get=get)\n",
    "\n",
    "parallel_estimate_pi(3000, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time estimate_pi(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time parallel_estimate_pi(10000000, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "%load solutions/Imperative-03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly faster than our original parallel implementation, and also runs faster than the serial version. The difference is even more apparent on larger sample sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time estimate_pi(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time parallel_estimate_pi(100000000, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
