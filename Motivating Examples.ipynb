{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#What is Dask and Why should I care?\n",
    "\n",
    "* Ever had a \"real\" dataset too big to analyze on your laptop when the test dataset worked fine?\n",
    "  * What did you do? \n",
    "    * Run it on a different server? \n",
    "    * Break it up into chunks manually and then aggregate the results?\n",
    "* Ever had a \"real\" dataset that took too long to analyze when the test dataset worked fine?\n",
    "  * What did you do?\n",
    "    * parallelize?\n",
    "    * rewrite to use numpy or numba?\n",
    "    \n",
    "Wouldn't it be nice if we had a tool that could chunk data and parallelize computations without having to add a lot of boilerplate?\n",
    "    \n",
    "##What is Dask?\n",
    "Dask is a way to structure computation for scalability and parallelism.\n",
    "With dask, we can structure computations to break down problems into digestible chunks that can also be run in parallel.\n",
    "\n",
    "####Let's see how array computations with Dask can make things easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chunk 0 -> 300\n",
      "Writing chunk 300 -> 600\n",
      "Writing chunk 600 -> 900\n",
      "Writing chunk 900 -> 1200\n",
      "Writing chunk 1200 -> 1500\n",
      "Writing chunk 1500 -> 1800\n",
      "Writing chunk 1800 -> 2100\n",
      "Writing chunk 2100 -> 2400\n",
      "Writing chunk 2400 -> 2700\n",
      "Writing chunk 2700 -> 3000\n"
     ]
    }
   ],
   "source": [
    "#Here is a \"black box\" of data\n",
    "import h5py\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "shape = 3000\n",
    "chunk = shape//10\n",
    "with h5py.File(\"testdata.hdf5\", \"w\") as f:\n",
    "    dataset = f.create_dataset(\"/data\", shape=(shape, shape), \n",
    "                               dtype=numpy.dtype('float64'), chunks = (chunk, shape))\n",
    "\n",
    "    for index in xrange(0, shape, chunk):\n",
    "        print \"Writing chunk\", index, \"->\", index+chunk\n",
    "        dataset[index:index+chunk] = numpy.random.rand(chunk, shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n",
      "100 loops, best of 3: 6.34 ms per loop\n",
      "100 loops, best of 3: 6.31 ms per loop\n",
      "The slowest run took 8.65 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 37 ms per loop\n",
      "10 loops, best of 3: 36.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "#Let's get some basic statistics about our data\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"testdata.hdf5\", \"r\")\n",
    "dataset = f['/data'][:]\n",
    "\n",
    "print dataset.shape\n",
    "%timeit dataset.sum()\n",
    "%timeit dataset.mean()\n",
    "%timeit dataset.var()\n",
    "%timeit dataset.std()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n",
      "10 loops, best of 3: 35.3 ms per loop\n",
      "10 loops, best of 3: 43.6 ms per loop\n",
      "10 loops, best of 3: 66.3 ms per loop\n",
      "10 loops, best of 3: 65.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "#How do we do that same thing in Dask?\n",
    "import h5py\n",
    "import dask\n",
    "import dask.array\n",
    "from pprint import pprint\n",
    "\n",
    "f = h5py.File(\"testdata.hdf5\", \"r\")\n",
    "dataset = f['/data']\n",
    "d = dask.array.from_array(dataset, dataset.chunks)\n",
    "\n",
    "print dataset.shape\n",
    "%timeit d.sum().compute()\n",
    "%timeit d.mean().compute()\n",
    "%timeit d.var().compute()\n",
    "%timeit d.std().compute()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300\n",
      "300 600\n",
      "600 900\n",
      "900 1200\n",
      "1200 1500\n",
      "1500 1800\n",
      "1800 2100\n",
      "2100 2400\n",
      "2400 2700\n",
      "2700 3000\n",
      "3000 3300\n",
      "3300 3600\n",
      "3600 3900\n",
      "3900 4200\n",
      "4200 4500\n",
      "4500 4800\n",
      "4800 5100\n",
      "5100 5400\n",
      "5400 5700\n",
      "5700 6000\n",
      "6000 6300\n",
      "6300 6600\n",
      "6600 6900\n",
      "6900 7200\n",
      "7200 7500\n",
      "7500 7800\n",
      "7800 8100\n",
      "8100 8400\n",
      "8400 8700\n",
      "8700 9000\n",
      "9000 9300\n",
      "9300 9600\n",
      "9600 9900\n",
      "9900 10200\n",
      "10200 10500\n",
      "10500 10800\n",
      "10800 11100\n",
      "11100 11400\n",
      "11400 11700\n",
      "11700 12000\n",
      "12000 12300\n",
      "12300 12600\n",
      "12600 12900\n",
      "12900 13200\n",
      "13200 13500\n",
      "13500 13800\n",
      "13800 14100\n",
      "14100 14400\n",
      "14400 14700\n",
      "14700 15000\n",
      "15000 15300\n",
      "15300 15600\n",
      "15600 15900\n",
      "15900 16200\n",
      "16200 16500\n",
      "16500 16800\n",
      "16800 17100\n",
      "17100 17400\n",
      "17400 17700\n",
      "17700 18000\n",
      "18000 18300\n",
      "18300 18600\n",
      "18600 18900\n",
      "18900 19200\n",
      "19200 19500\n",
      "19500 19800\n",
      "19800 20100\n",
      "20100 20400\n",
      "20400 20700\n",
      "20700 21000\n",
      "21000 21300\n",
      "21300 21600\n",
      "21600 21900\n",
      "21900 22200\n",
      "22200 22500\n",
      "22500 22800\n",
      "22800 23100\n",
      "23100 23400\n",
      "23400 23700\n",
      "23700 24000\n",
      "24000 24300\n",
      "24300 24600\n",
      "24600 24900\n",
      "24900 25200\n",
      "25200 25500\n",
      "25500 25800\n",
      "25800 26100\n",
      "26100 26400\n",
      "26400 26700\n",
      "26700 27000\n",
      "27000 27300\n",
      "27300 27600\n",
      "27600 27900\n",
      "27900 28200\n",
      "28200 28500\n",
      "28500 28800\n",
      "28800 29100\n",
      "29100 29400\n",
      "29400 29700\n",
      "29700 30000\n"
     ]
    }
   ],
   "source": [
    "#Yay! So we just took a simple computation and ...\n",
    "#  added more code that makes it more complicated,\n",
    "#  less understandable,\n",
    "#  and runs slower!\n",
    "#Exactly what I wanted!\n",
    "\n",
    "#Let's do it again on our real data that is a little bit bigger\n",
    "\n",
    "import h5py\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "shape = 30000\n",
    "chunk = shape//100\n",
    "with h5py.File(\"realdata.hdf5\", \"w\") as f:\n",
    "    dataset = f.create_dataset(\"/data\", shape=(shape, shape), \n",
    "                               dtype=numpy.dtype('float64'), chunks = (chunk, shape))\n",
    "\n",
    "    for index in xrange(0, shape, chunk):\n",
    "        print \"Writing chunk\", index, \"->\", index+chunk\n",
    "        dataset[index:index+chunk] = numpy.random.rand(chunk, shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import dask.array\n",
    "import dask.threaded\n",
    "import dask.multiprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "f = h5py.File(\"realdata.hdf5\", \"r\")\n",
    "dataset = f['/data']\n",
    "d = dask.array.from_array(dataset, dataset.chunks)\n",
    "\n",
    "pprint(d.sum().dask)\n",
    "#print dask.threaded.get(d.sum().dask, ('x_423', 0, 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pprint(d.sum().dask)\n",
    "dsum = d.sum().dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4500429.06907939]]\n"
     ]
    }
   ],
   "source": [
    "pprint(dsum)\n",
    "print dask.threaded.get(dsum, ('x_17', 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449999202.82\n",
      "0.499999114244\n",
      "6.39423419252e-10\n",
      "(30000, 30000) (300, 30000)\n"
     ]
    }
   ],
   "source": [
    "#Our original, easy-to-understand code doesn't work on bigger problems.\n",
    "#How do we fix that?\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"realdata.hdf5\",\"r\")\n",
    "dataset = f['/data']\n",
    "chunksize = f['/data'].chunks[0]\n",
    "\n",
    "final_sum = []\n",
    "for chunk_start_idx in xrange(0, dataset.shape[0], chunksize):\n",
    "    final_sum.append(dataset[chunk_start_idx:chunk_start_idx+chunksize].sum())\n",
    "print np.array(final_sum).sum()\n",
    "\n",
    "final_mean = []\n",
    "for chunk_start_idx in xrange(0, dataset.shape[0], chunksize):\n",
    "    final_mean.append(dataset[chunk_start_idx:chunk_start_idx+chunksize].mean())\n",
    "print np.array(final_mean).mean()\n",
    "\n",
    "#I'm not completely clear on how we compose a correct variance from chunks\n",
    "#I'll have to think about it a little.\n",
    "#If only there were a tool that did that for me!\n",
    "final_var = []\n",
    "for chunk_start_idx in xrange(0, dataset.shape[0], chunksize):\n",
    "    final_var.append(dataset[chunk_start_idx:chunk_start_idx+chunksize].var())\n",
    "print np.array(final_var).var()\n",
    "\n",
    "\n",
    "print dataset.shape, f['/data'].chunks\n",
    "#print dataset[:].sum()\n",
    "#print dataset[:].mean()\n",
    "#print dataset[:].var()\n",
    "#print dataset[:].std()\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Discussion\n",
    "The example code that breaks the input into chunks, computes each chunk, and then composes the intermediate results into a final answer is exactly what dask does. \n",
    "Our original code worked well for a small dataset that fit into memory.\n",
    "Our original code did not work when the dataset was too large for memory.\n",
    "So we had to jump through hoops to partition things.\n",
    "\n",
    "Something we missed is that dask does things in parallel for us.\n",
    "Each of the intermediate mean() results could have been calculated in parallel.\n",
    "But we didn't add that functionality.\n",
    "\n",
    "Dask has it builtin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let's see how Dask.dataframe can make things easier\n",
    "<Come up with a decent example>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
